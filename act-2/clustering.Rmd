---
title: "Clustering en R"
author: "Andrés Marrugo, PhD"
date: "7/9/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introducción al clustering en R

Basado en este [ejemplo](https://rpubs.com/rdelgado/399475) por R. Delgado.

Aquí exploraremos uno de los métodos de clustering (o agrupamiento) más populares en Machine Learning: el K-means. Como vimos en la presentación, este método es considerado no supervisado ya que no se conocen los grupos o etiquetas de los datos.

## Data set

Usaremos un [dataset](https://cs.joensuu.fi/sipu/datasets/) bidimensional generado artificialmente. 

Carguemos los datos y visualicemos.

```{r}
dataset <- read.csv('AggregationClusters.csv')
str(dataset)
```

```{r}
summary(dataset)
```

Como podemos ver a partir de la información del dataset, está compuesto por 788 observaciones de 2 variables que abarcan un amplio rango numérico. Ya que se trata de un conjunto bidimensional, podemos graficar los datos en un scatterplot a fin de observar su distribución de manera más clara. Para ello, haremos uso de la librería ``ggplot2``:

```{r}
library(ggplot2)
ggplot() + geom_point(aes(x = X, y = Y), data = dataset, alpha = 0.5) + ggtitle('Conjunto de Datos')
```

Evidentemente, en el data-set existen unos 5 a 7 grupos que se distribuyen en zonas particulares del rango de valores de las variables. El objetivo del clustering es identificar de manera precisa estos grupos. Además, nos interesa determinar las fronteras de cada uno, de manera que una observación nueva pueda ser asociada a una clase específica. 

## K-means clustering

Como vimos en las diapositivas, el método de K-means agrupa los datos de entrada en un total de k conjuntos definidos por un centroide cuya distancia con los puntos que pertenecen a cada uno de los datos es la menor posible. 

Por otra parte, la cantidad óptima de centroides k a utilizar no necesariamente se conoce de antemano, por lo que se pueden aplicar heurísticas y métodos como el Elbow Method a fin de determinar dicho valor. Básicamente, este método busca seleccionar la cantidad ideal de grupos a partir de la optimización de la WCSS (Within Clusters Summed Squares).

Adicionalmente, ya que los centroides iniciales se generan al azar, pueden obtenerse resultados distintos en cada corrida del algoritmo, e incluso debido a las ubicaciones iniciales de los centroides, obtenerse al final soluciones que son mínimos locales en vez del global real del conjunto de datos. Para solventar este problema, se propuso el algoritmo de k-means++ a fin de escoger los centroides iniciales que garantizaran la convergencia adecuada del modelo.

Entonces, a fin de implementar el modelo de K-means, comencemos por determinar la cantidad óptima de centroides a utilizar a partir del Elbow method. Para ello, aplicaremos la función kmeans al conjunto de datos, variando en cada caso el valor de k, y acumulando los valores de WCSS obtenidos:

```{r}
set.seed(1234)
wcss <- vector()
for(i in 1:20){
  wcss[i] <- sum(kmeans(dataset, i)$withinss)
}
```

Una vez calculados los valores de WCSS en función de la cantidad de centroides k, vamos a graficar los resultados:

```{r}
ggplot() + geom_point(aes(x = 1:20, y = wcss), color = 'blue') + 
  geom_line(aes(x = 1:20, y = wcss), color = 'blue') + 
  ggtitle("Método del Codo") + 
  xlab('Cantidad de Centroides k') + 
  ylab('WCSS')
```

A partir de la curva obtenida podemos ver cómo a medida que se aumenta la cantidad de centroides, el valor de WCSS disminuye de tal forma que la gráfica adopta una forma de codo. Para seleccionar el valor óptimo de k, se escoje entonces ese punto en donde ya no se dejan de producir variaciones importantes del valor de WCSS al aumentar k. En este caso, vemos que esto se produce a partir de k >= 7, por lo que podemos evaluar los resultados del agrupamiento, por ejemplo, con los valores de 7, 8 y 9 a fin de observar el comportamiento del modelo.

Finalmente, podemos aplicar el algoritmo con la cantidad de k seleccionada:

```{r}
set.seed(1234)
kmeans <- kmeans(dataset, 7, iter.max = 1000, nstart = 10)
```


En donde iter.max son el máximo de iteraciones a aplicar al algoritmo, y nstart es la cantidad de conjuntos de centroides que emplea internamente el mismo para ejecutar sus cálculos.

Veamos el resultado del agrupamiento:

```{r}
dataset$cluster <- kmeans$cluster
ggplot() + geom_point(aes(x = X, y = Y, color = cluster), data = dataset, size = 2) +
  scale_colour_gradientn(colours=rainbow(4)) +
  geom_point(aes(x = kmeans$centers[, 1], y = kmeans$centers[, 2]), color = 'black', size = 3) + 
  ggtitle('Clusters de datos con k = 7 / K-Means') + 
  xlab('X') + ylab('Y')
```

En la gráfica se representa cada cluster con un color diferente, y además se muestra la posición de cada centroide en negro.

Como puede verse, con k = 7 el modelo asigna clases consistentes a los datos de entrada, en especial al observar los agrupamientos que existen en toda la zona superior y derecha de la gráfica en donde los grupos son evidentes. Mientras tanto, los datos del grupo inferior izquierdo son agrupados en 3 clases distintas pero se observa que la distribución de puntos es adecuada para cada grupo.

Al validar con k = 8 y 9 tenemos:

```{r}
set.seed(1234)
kmeans <- kmeans(dataset, 8, iter.max = 1000, nstart = 10)
dataset$cluster <- kmeans$cluster
ggplot() + geom_point(aes(x = X, y = Y, color = cluster), data = dataset, size = 2) +
  scale_colour_gradientn(colours=rainbow(4)) +
  geom_point(aes(x = kmeans$centers[, 1], y = kmeans$centers[, 2]), color = 'black', size = 3) + 
  ggtitle('Clusters de Datos con k = 8 / K-Medios') + 
  xlab('X') + ylab('Y')
```

```{r}
set.seed(1234)
kmeans <- kmeans(dataset, 9, iter.max = 1000, nstart = 10)
dataset$cluster <- kmeans$cluster
ggplot() + geom_point(aes(x = X, y = Y, color = cluster), data = dataset, size = 2) +
  scale_colour_gradientn(colours=rainbow(4)) +
  geom_point(aes(x = kmeans$centers[, 1], y = kmeans$centers[, 2]), color = 'black', size = 3) + 
  ggtitle('Clusters de Datos con k = 9 / K-Medios') + 
  xlab('X') + ylab('Y')
```

Al incrementar el valor de k tendremos entonces agrupamientos que recogen partes específicas de los datos de entrada, incluso llegando a dividir en dos grupos distintos a lo que inicialmente parece ser un solo grupo, como es el caso de los datos presentes en la parte superior izquierda de la gráfica cuando k = 9. Así, vemos cómo el algoritmo de K-Medios es capas de producir de manera natural estos agrupamientos a partir de las semejanzas de los datos, y dichas clases generadas de hecho concuerdan con la intuición propia al observar los datos de entrada.




